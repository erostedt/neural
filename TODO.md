* Use adam optimizer

* Add activation SOFTMAX

* Add binary crossentropy loss
* Add catagorical crossentropy loss

* Make forwarding after training not requiring same batch size as training?

* Make layer_spec not take input size
* Simplify construction of NN

* Clean up repo structure
* Remove unnecessary functions

* #pragma omp parallel for?
